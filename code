import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random as r
from keras.models import Sequential,load_model,Model,model_from_json
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D,concatenate, Conv2D, MaxPooling2D, Conv2DTranspose
from keras.layers import Input, merge, UpSampling2D
from keras.callbacks import ModelCheckpoint
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img

from google.colab import files
uploaded = files.upload()

import io
df2 = pd.read_csv(io.BytesIO(uploaded['survival_data.csv']))
#K.set_image_data_format("channels_first")
K.set_image_dim_ordering("th")
img_size = 120 #original img size is 240*240
smooth = 1 
num_of_aug = 1
num_epoch = 70

import glob
def create_data(src, mask, label=False, resize=(155,img_size,img_size)):
files = glob.glob(src + mask, recursive=True)
imgs = []
print('Processing---', mask)
for file in files:
img = io.imread(file, plugin='simpleitk')
img = trans.resize(img, resize, mode='constant')

 def augmentation(scans,n): #input img must be rank 4 
datagen = ImageDataGenerator(
featurewise_center=False, 
samplewise_center=False, 
featurewise_std_normalization=False, 
samplewise_std_normalization=False, 
rotation_range=25, 
#width_shift_range=0.3, 
#height_shift_range=0.3, 
horizontal_flip=True, 
vertical_flip=True, 
zoom_range=False)
i=0
scans_g=scans.copy()
for batch in datagen.flow(scans, batch_size=1, seed=1000): 
scans_g=np.vstack([scans_g,batch])
i += 1
if i == n:
labels_g=labels.copy()
if i > n:
break 
return ((survival_g,labels_g))'''
return scans_g
#scans_g,labels_g = augmentation(img,img1, 10)
#X_train = X_train.reshape(X_train.shape[0], 1, img_size, img_size)

#%%
'''
Model -
structure:
''' 
def dice_coef(y_true, y_pred):
y_true_f = K.flatten(y_true)
y_pred_f = K.flatten(y_pred)
intersection = K.sum(y_true_f * y_pred_f)
return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)
def dice_coef_loss(y_true, y_pred):
return -dice_coef(y_true, y_pred)
def unet_model():
inputs = Input((1, img_size, img_size))
conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs) # KERNEL =3 STRIDE =3
conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv1)
pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(pool1)
conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv2)
pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(pool2)
conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv3)
pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(pool3)
conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv4)
pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)
conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(pool4)
conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(conv5)
up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1
conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(up6)
conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv6)
up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)
conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(up7)
conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv7)
up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)
conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(up9)
conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv9)
conv10 = Convolution2D(1, 1, 1, activation='sigmoid')(conv9)
model = Model(input=inputs, output=conv10)
model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])
return model

#%%
# catch all T1c.mha
create_data('/home/hrps/Brain_tumor/MICCAI_BraTS_2018_Data_Training/HGG/', label=False, resize=(155,img_size,img_size))
create_data('/home/hrps/Brain_tumor/MICCAI_BraTS_2018_Data_Training/HGG/', label=True, resize=(155,img_size,img_size))
create_data('/home/hrps/Brain_tumor/MICCAI_BraTS_2018_Data_Training/', '**/*Brats18_2013_2_1_flair.nii.gz', label=False, resize=(155,img_size,img_size))
create_data('/home/hrps/Brain_tumor/MICCAI_BraTS_2018_Data_Training/', '**/*_Brats18_2013_2_1_flair.nii.gz', label=True, resize=(155,img_size,img_size))
#%%


# load numpy array data
x = np.load('/home/hrps/x_{}.npy'.format(img_size))
y = np.load('/home/hrps/y_{}.npy'.format(img_size))
#%%
#training
num = 51000
model = unet_model()
history = model.fit(x, y, batch_size=16, validation_split=0.2 ,nb_epoch= num_epoch, verbose=1, shuffle=True)
pred = model.predict(x[num:num+100])
# save model and weights
model.save('aug{}_{}_epoch{}'.format(num_of_aug,img_size,num_epoch))
model.save_weights('weights_{}_{}.h5'.format(img_size,num_epoch))
#model.load_weights('weights.h5')
print(history.history.keys())

plt.plot(history.history['dice_coef'])
plt.plot(history.history['val_dice_coef'])
plt.title('model dice_coef')
plt.ylabel('dice_coef')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()


for n in range(2):
i = int(r.random() * pred.shape[0])
plt.figure(figsize=(15,10))
plt.subplot(131)
plt.title('Input'+str(i+num))
plt.imshow(x[i+num, 0, :, :],cmap='gray')
plt.subplot(132)
plt.title('Ground Truth')
plt.imshow(y[i+num, 0, :, :],cmap='gray')
plt.subplot(133)
plt.title('Prediction')
plt.imshow(pred[i, 0, :, :],cmap='gray')
plt.show()







